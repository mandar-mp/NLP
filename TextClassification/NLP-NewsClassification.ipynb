{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_news(index,news):\n",
    "#     print(news)\n",
    "    wordlist= [word  for sent in nltk.tokenize.sent_tokenize(news) for word in nltk.tokenize.word_tokenize(sent)]\n",
    "#     print('words in news ', index,' = ', len(wordlist))\n",
    "    \n",
    "    #remove stopwords\n",
    "    stopwordlist = stopwords.words('english')\n",
    "   \n",
    "    wordlist = [i for i in wordlist if i not in stopwordlist]\n",
    "#     print('words after stopword removal in news ', index,' = ', len(wordlist))\n",
    "    \n",
    "    # remove words less than or equal to 2 characters\n",
    "    \n",
    "    wordlist = [ i for i in wordlist if len(i) >2 ]\n",
    "#     print('words after small words removal in news ', index,' = ', len(wordlist))\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    wordlist = [lmtzr.lemmatize(word) for word in wordlist]\n",
    "    \n",
    "    wordlist=set(wordlist)\n",
    "    preprocessedText= ' '.join(wordlist)\n",
    "    return preprocessedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdata=pd.read_csv('NlpNewsClassification/bbc-text.csv')\n",
    "\n",
    "# print(newsdata.shape)\n",
    "processed_news = []\n",
    "newscategory = []\n",
    "for index,row in newsdata.iterrows():\n",
    "#     print(row['text'])\n",
    "    newstext = str(row['text'])\n",
    "    newscategory.append(row['category'])\n",
    "    \n",
    "#     print('type(newstext)=',type(newstext))\n",
    "#     print( newstext)\n",
    "    processed_news.append(preprocess_news(index,newstext))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1557,) (668,) (1557,) (668,)\n"
     ]
    }
   ],
   "source": [
    "# prepare feature set and train test set\n",
    "featureset=pd.DataFrame()\n",
    "\n",
    "featureset['text'] = processed_news\n",
    "featureset['label'] = newscategory\n",
    "featureset.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(featureset['text'],featureset['label'], test_size=0.30, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1557, 26318) (668, 26318) (1557,) (668,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfIdfVectorizer = vectorizer.fit(featureset['text'])\n",
    "X_train_tfidf = tfIdfVectorizer.transform(X_train)\n",
    "X_test_tfidf = tfIdfVectorizer.transform(X_test)\n",
    "\n",
    "# label encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(featureset['label'])\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(X_train_tfidf.shape, X_test_tfidf.shape, y_train_encoded.shape, y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, train_label, feature_vector_valid, valid_label):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, train_label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "        \n",
    "    return accuracy_score(predictions, valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.9431137724550899\n"
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "accuracy = train_model(MultinomialNB(), X_train_tfidf, y_train_encoded, X_test_tfidf ,y_test_encoded )\n",
    "print('NB, Count Vectors: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
